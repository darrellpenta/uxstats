% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/success_vs_bench-function.R
\name{success_vs_bench}
\alias{success_vs_bench}
\alias{completion_vs_bench}
\alias{success_vs_bench.numeric}
\alias{success_vs_bench.data.frame}
\title{Compare a success rate to a benchmark}
\usage{
success_vs_bench(.x, ...)

completion_vs_bench(.x, ...)

\method{success_vs_bench}{numeric}(
  .x,
  .n = NULL,
  .p,
  ...,
  .alt = c("greater", "less", "twotailed"),
  .alpha = 0.05
)

\method{success_vs_bench}{data.frame}(
  .x,
  .var,
  .p,
  ...,
  .alt = c("greater", "less", "twotailed"),
  .alpha = 0.05
)
}
\arguments{
\item{.x}{A single numeric value, a vector of values, or a long-format data frame with a named column of numeric data (1s and/or 0s) corresponding to task success outcomes. See Details.}

\item{...}{(Optional) If \code{.x} is a long-format data frame, you can pass the name of one or more grouping variables as unquoted, comma-separated column names (without naming the \code{...} argument) to compute stats by groups.}

\item{.n}{A single numeric value representing the total number of trials. See Details.}

\item{.p}{The test (benchmark) proportion (must be a numeric between 0-1).}

\item{.alt}{For test alternatives, one of \code{c("greater","less","twotailed")}. Defaults to "greater" for a one-sided test.}

\item{.alpha}{(Optional) A positive number (where 0 < \code{.alpha} < 1) specifying the significance level to be used. Defaults to \code{.alpha = 0.05}. To set a different significance level, the argument must be named (i.e., \code{.alpha=0.001}) or else the function may yield unexpected results.}

\item{.var}{If \code{.x} is a long-format data frame, the (unquoted) name of a data frame column containing task success outcomes (as 1s and 0s, corresponding to successes and failures, respectively).}
}
\value{
A tibble with data summaries and test results
}
\description{
\code{success_vs_bench()} tests an observed success rate against a given benchmark. Following \href{https://g.co/kgs/a7Zyyn}{Sauro and Lewis (2012)}, it takes the sample size into account in providing estimates.

\code{success_vs_bench()} and \code{completion_vs_bench()} are synonyms.
}
\details{
\code{success_vs_bench()} returns a variety of estimates. \href{https://g.co/kgs/a7Zyyn}{Sauro and Lewis (2012)} recommend using the mid-probability from the binomial distribution for small sample sizes (i.e., cases with fewer than 15 successes and 15 failures), and for large sample sizes, using the normal approximation to the binomial. The function also reports the best estimate success rate using the Laplace calculation.

\code{success_vs_bench} assumes that you want to test the hypothesis that the observed outcome \emph{exceeds} the benchmark, and therefore, defaults to a one-tailed test. This means that setting \code{.alpha = 0.05} (the default) produces a 90\% confidence interval.

\itemize{
  \item If \code{.x} is a single numeric value representing the total number of successes, \code{.n} should be a single numeric value representing the total number of users, where the value of \code{.n} >= the value of \code{.x}). e.g., \code{.x = 23, .n = 25}
  \item If \code{.x} is a data frame, \code{.var} should be the unquoted name of the column containing the success data (as 1s and 0s).
  \item If you're passing a data frame to \code{.x}, you can optionally pass one or more grouping variables as unquoted, comma-separated column names (without naming the \code{...} argument) to compute stats by groups.
  \item You can choose from among the test alternatives \code{c("greater","less","twotailed")} by providing one of the options to the \code{.alt} argument: e.g., \code{.alt = "twotailed"}. Defaults to "greater" for a one-sided test.
  \item You can modify the alpha level to adjust confidence intervals by including \code{.alpha} as a named argument and providing a numeric value: e.g., \code{.aplha = 0.001}.
}

Note that \code{NAs} are automatically dropped in all calculations.
}
\examples{
# Comparing 19 success/25 trials (users) to a 75\% benchmark completion rate
success_vs_bench(19,25,0.75)

.ux_data <-
data.frame(
 "id" = rep(seq(1,10,1),2),
 "task" = c(rep(1,10),rep(2,10)),
 "complete"  = sample(0:1,20,replace=TRUE,prob = c(.3,.65))
)

success_vs_bench(.ux_data, complete, .p=0.7,task)
}
\seealso{
Other benchmark comparison stats: 
\code{\link{ratings_vs_bench}()},
\code{\link{time_vs_bench}()}
}
\concept{benchmark comparison stats}
